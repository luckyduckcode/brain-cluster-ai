# ğŸ§  Digital Cortex - Bio-Mimetic AGI System

A brain-inspired multi-agent AI architecture using local LLMs as "neurons" and specialized neural networks as "brain regions."

## ğŸ¯ What We've Built (Major Milestones Complete!)

### âœ… **Digital Body Architecture - COMPLETE**
**Status:** 100% Complete

**What was built:**
- **Container Architecture**: Multi-container pod with sensory/motor/brain/autonomic containers
- **Message Bus**: Inter-container communication for sensory-motor-brain feedback loops
- **Health Monitoring**: Resource usage, component status, auto-regulation
- **Digital Body Integration**: Complete embodied AI system with all brain regions

### âœ… **YouTube Learning System - COMPLETE**
**Status:** 100% Complete

**What was built:**
- **Video Acquisition**: YouTube download, frame extraction, audio processing
- **Multimodal Sensorium**: Vision (LLaVA), Audio (Whisper STT), Text (captions/OCR)
- **Learning Orchestration**: Parallel processing, consensus synthesis, knowledge extraction
- **Memory Integration**: Store concepts, facts, procedures in Memory Palace
- **Knowledge Retrieval**: Query and apply learned video knowledge
- **Container Integration**: Video processing container with GPU acceleration

**Usage:**
```bash
# Learn from a YouTube video
"learn from video: https://youtube.com/watch?v=VIDEO_ID"

# Query learned knowledge
"what do you know about machine learning?"
"find videos about neural networks"
"learning stats"
```

### âœ… Core Components Implemented

1. **Message Protocol** (`utils/message.py`)
   - Standardized communication format for all components
   - JSON-serializable with source, content, confidence, and timestamp
   - Validation and factory methods

2. **Corpus Colosseum** (`corpus_colosseum/colosseum.py`)
   - Short-term working memory with consensus mechanism
   - DBSCAN clustering for finding convergence among outputs
   - Weighted voting based on confidence scores
   - Automatic reset and capacity management

3. **LLM-Neuron Interface** (`utils/llm_neuron.py`)
   - Connects local Ollama models as processing neurons
   - Confidence score extraction from LLM outputs
   - NeuronPool for managing multiple specialized neurons
   - Support for different system prompts and temperatures

4. **Integration Demo** (`demo_integration.py`)
   - Complete end-to-end demonstration
   - Snake vs Garden Hose scenario from white paper
   - 4 specialized neurons (2 threat-focused, 2 logic-focused)
   - Real-time consensus finding

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull required models
ollama pull llama3.2:1b          # Main reasoning model
ollama pull llava:7b             # Vision processing (for video learning)

# Install system dependencies (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install ffmpeg       # For video processing

# Install Python dependencies
pip install -r requirements.txt
```

**Additional Models for Full Functionality:**
- `llava:7b` - Vision-language model for video frame analysis
- `whisper` - Audio transcription (handled by yt-dlp integration)

### ğŸ–¥ï¸ **Standalone Desktop App - NEW!**
**Status:** Complete - Simple & User-Friendly

**What was built:**
- **Clean Interface**: Simple, welcoming desktop app focused on chat
- **Easy Chat**: Large chat area with clear message formatting
- **Smart Initialization**: Background brain loading with status updates
- **User-Friendly Design**: No complex tabs, just chat and status
- **Modern UI**: CustomTkinter with automatic themes

**Features:**
- ğŸ’¬ **Simple Chat Interface**: Clean, easy-to-use chat with Chappy
- ğŸ§  **Smart AI Companion**: Brain-inspired multimodal AI with personality
- ğŸ¨ **Modern UI**: Automatic dark/light theme with friendly design
- âš¡ **Fast Startup**: Quick initialization with clear status messages
- ğŸ¯ **User-Friendly**: No complex tabs or technical jargon

**Usage:**
```bash
# Install dependencies
pip install -r requirements.txt

# Launch the simple desktop app
python3 launch_chappy_standalone.py

# Or run directly
python3 chappy_standalone_simple.py

# Test the installation
python3 test_standalone.py
```

**What You'll See:**
- **Welcome Screen**: Friendly greeting and introduction
- **Chat Area**: Large, clear chat window for conversations
- **Input Box**: Simple text entry with big "Send" button
- **Status Bar**: Clear messages about Chappy's brain status

**Perfect for:**
- First-time AI users
- Casual conversations with AI
- Educational demonstrations
- Quick AI interactions

**Features:**
- ğŸ’¬ Real-time chat with Chappy's multimodal brain
- ğŸ¥ One-click YouTube video learning with progress tracking
- ğŸ§  Memory palace visualization and management
- ğŸ“Š Live system monitoring and brain status
- ğŸ¨ Modern UI with automatic dark/light theme detection
- âŒ¨ï¸ Keyboard shortcuts (Ctrl+N for new chat, Ctrl+Q to quit)
- ğŸ’¾ Conversation saving and export capabilities
- ğŸ”„ Brain restart and settings management

**System Requirements:**
- Python 3.12+
- 4GB RAM minimum, 8GB recommended
- Ollama with llama3.2:1b model
- For video learning: ffmpeg, yt-dlp, OpenCV

**Desktop Integration (Linux):**
```bash
# Copy desktop entry to applications
cp chappy.desktop ~/.local/share/applications/

# Create icon (256x256 PNG recommended)
# Save as: chappy_icon.png in project root
# Then search for "Chappy AI" in your app launcher
```

### ğŸŒ Web Interface (Legacy)
```bash
# Install dependencies
pip install -r requirements.txt

# Launch web interface
python3 launch_chappy.py
# Then open http://localhost:8501
```

### REST API
```bash
# Install dependencies (includes FastAPI)
pip install -r requirements.txt

# Launch REST API
python3 launch_api.py
# or
./launch_api.sh
# API available at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

### Observability Dashboard
```bash
# Install dependencies (includes Streamlit, Plotly)
pip install -r requirements.txt

# Launch observability dashboard (requires API to be running)
python3 launch_dashboard.py
# or
./launch_dashboard.sh
# Dashboard available at http://localhost:8501
# Make sure API server is running on http://localhost:8000
```

### Setup
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # or: ./venv/bin/activate

# Install dependencies (includes video processing libraries)
pip install -r requirements.txt

# Install additional video processing dependencies
pip install yt-dlp opencv-python

# For GPU acceleration (optional)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Run Tests
```bash
# Test Corpus Colosseum (no LLM required)
./venv/bin/python digital_cortex/tests/test_colosseum.py

# Test LLM-Neuron connectivity
./venv/bin/python digital_cortex/test_neuron_quick.py

# Run full integration demo (requires Ollama)
./venv/bin/python digital_cortex/demo_integration.py
```

### ğŸ¨ Chappy the Brain Cluster GUI

Experience Chappy thinking out loud with an interactive web interface!

```bash
# Install additional GUI dependencies
pip install streamlit>=1.28.0

# Launch Chappy's GUI
python launch_chappy.py
```

### ğŸ–¥ï¸ Chappy Desktop App

Run Chappy as a standalone desktop application with his own native window!

**Features:**
- Native desktop window (no browser required)
- Auto-starts Chappy's brain server
- Embedded web interface in desktop app
- Cross-platform support (Linux, Mac, Windows)
- Auto-prompt feature (Chappy thinks when idle)
- Memory integration and live thought streaming

**Quick Launch:**
```bash
# Linux/Mac
./launch_chappy_desktop.sh

# Windows
launch_chappy_desktop.bat

# Or directly
python3 chappy_desktop.py
```

**System Requirements:**
- Python 3.8+
- Ollama running with llama3.2:1b model
- Desktop environment (X11, Wayland, or Windows)

**Features:**
- ğŸ—£ï¸ **Real-time chat** with Chappy
- ğŸ§  **Live thought stream** showing brain activity
- ğŸ“Š **Brain status monitor** with component health
- ğŸ¯ **Executive decisions** and meta-cognition display
- ğŸ’­ **Memory palace** visualization
- ğŸ”´ **Live processing** through all brain regions

**What Chappy Can Do:**
- Answer questions and hold conversations
- Show his thought process through each brain region
- Learn from interactions and remember conversations
- Make executive decisions when faced with uncertainty
- Express emotions and assess situations
- **ğŸ¥ Watch and learn from YouTube videos** through multimodal processing
- **ğŸ“š Recall and apply knowledge** learned from educational videos
- **ğŸ§  Build knowledge base** from video content with structured extraction

## ğŸŒ REST API

Integrate Chappy into your applications with a full REST API!

**Quick Launch:**
```bash
# Install dependencies
pip install -r requirements.txt

# Launch API server
python3 launch_api.py
# or
./launch_api.sh

# API available at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

**API Endpoints:**

### `POST /api/v1/query`
Process a query through Chappy's brain.

**Request:**
```json
{
  "query": "What is the meaning of life?",
  "max_memories": 5,
  "include_thoughts": false
}
```

**Response:**
```json
{
  "response": "My friend, the meaning of life...",
  "confidence": 0.85,
  "processing_time": 2.34,
  "memory_count": 3,
  "consensus_reached": true
}
```

### `GET /api/v1/status`
Get system status and health information.

### `GET /api/v1/memories`
Retrieve recent memories from Chappy's memory palace.

### `POST /api/v1/feedback`
Provide feedback on responses to help Chappy learn.

**Features:**
- ğŸš€ **FastAPI** framework with automatic OpenAPI docs
- ğŸ“Š **Real-time processing** through all brain regions
- ğŸ§  **Memory integration** for contextual responses
- ğŸ’¬ **Thought process** optional detailed output
- ğŸ”„ **CORS enabled** for web integration
- ğŸ“ˆ **Performance metrics** and processing times

## ğŸ“Š Observability Dashboard

Monitor Chappy's brain activity in real-time with a comprehensive dashboard!

**Quick Launch:**
```bash
# Install dependencies (includes Streamlit, Plotly)
pip install -r requirements.txt

# Launch dashboard (requires API server running)
python3 launch_dashboard.py
# or
./launch_dashboard.sh

# Dashboard available at http://localhost:8501
```

**Dashboard Features:**
- ğŸ“ˆ **System Overview**: Uptime, query count, cache performance, response times
- ğŸ§  **Brain Activity**: Live neuron activity and consensus confidence trends
- ğŸ’¾ **Memory Network**: Memory count, connections, and growth visualization
- âš ï¸ **Health Monitoring**: API connectivity, performance indicators, and alerts
- ğŸ“Š **Interactive Charts**: Real-time Plotly visualizations with live updates
- ğŸ”„ **Auto-refresh**: Continuous monitoring every 2 seconds

**What You Can Monitor:**
- Real-time consensus decision processes
- Memory palace network growth and connectivity
- Neuron performance and health status
- Cache hit rates and response times
- System uptime and query throughput
- Alert notifications for issues

## ğŸ› ï¸ Tool Integration

Chappy can now use external tools to enhance his problem-solving capabilities!

**Automatic Tool Usage:**
Chappy automatically detects when queries require tools and uses them intelligently:
- **Mathematical queries** â†’ Calculator tool
- **Search requests** â†’ Web search tool  
- **Code execution** â†’ Code execution tool
- **Knowledge queries** â†’ Knowledge base tool

**Available Tools:**
- **ğŸ§® Calculator**: Symbolic math, equations, and calculations using SymPy
- **ğŸŒ Web Search**: Current information via DuckDuckGo API
- **ğŸ’» Code Execution**: Safe Python, JavaScript, and Bash execution
- **ğŸ“š Knowledge Base**: Math facts, unit conversions, structured data

**Direct Tool Access:**
```bash
# List available tools
curl http://localhost:8000/api/v1/tools

# Execute a tool directly
curl -X POST http://localhost:8000/api/v1/tools/execute \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "calculator", "params": {"expression": "2**10 + sqrt(144)"}}'
```

**Safety Features:**
- Sandboxed code execution
- Input validation and sanitization
- Dangerous operation blocking
- Timeout protection
- Restricted system access

## ğŸ¥ YouTube Learning System

Chappy can now watch and learn from educational YouTube videos!

**Features:**
- **ğŸ¬ Video Acquisition**: Downloads YouTube videos and extracts frames
- **ğŸ§  Multimodal Processing**: Parallel vision (LLaVA), audio (Whisper), and text analysis
- **ğŸ“š Knowledge Extraction**: Uses LLM to synthesize structured knowledge from videos
- **ğŸ§  Memory Integration**: Stores learned concepts in the Memory Palace
- **ğŸ” Knowledge Retrieval**: Query and recall learned video content
- **ğŸ“Š Learning Statistics**: Track videos processed and knowledge extracted

**Usage Examples:**
```bash
# Teach Chappy about machine learning
"learn from video: https://youtube.com/watch?v=VIDEO_ID"

# Ask about learned topics
"what do you know about neural networks?"
"find videos about artificial intelligence"
"learning stats"
```

**Technical Details:**
- Processes videos at 1 FPS for temporal analysis
- Uses Ollama LLaVA for vision understanding
- Whisper for speech-to-text transcription
- LLM synthesis for structured knowledge extraction
- Full integration with existing brain consensus mechanisms

## ğŸ“Š Demo Output Example

```
ğŸ§  Amygdala_Threat (confidence: 0.50)
   Response: I recommend taking caution and prioritizing safety...

ğŸ§  Logic_Classifier (confidence: 0.50)
   Response: I would classify the object as a "Snake"...

ğŸ† CONSENSUS REACHED
   Winning Neuron: Amygdala_Threat
   Confidence: 0.50
   Decision: Taking caution and prioritizing safety
```

## ğŸ¥ YouTube Learning Demo

```bash
# Run the video learning demo
python3 demo_video_learning.py

# Example output:
ğŸ¥ Chappy's YouTube Learning System Demo
==================================================
ğŸ§  Initializing brain components...
ğŸ¬ Initializing video learning container...
âœ… Video learning system ready!

ğŸ’¬ User: learning stats
ğŸ¯ Chappy: ğŸ“Š Video Learning Statistics:
â€¢ Videos Processed: 0
â€¢ Total Learning Time: 0.0 seconds
â€¢ Average Confidence: 0.00
â€¢ Knowledge Items Extracted: 0

ğŸ’¬ User: what do you know about machine learning?
ğŸ¯ Chappy: ğŸ“š Based on my video learning:
[Chappy synthesizes knowledge from learned videos...]

ğŸ¬ To learn from a YouTube video, use:
ğŸ’¬ 'learn from video: https://youtube.com/watch?v=VIDEO_ID'
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DIGITAL CORTEX SYSTEM                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         DIGITAL BODY CONTAINERS         â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚  SENSORY    â”‚ â”‚   MOTOR     â”‚        â”‚    â”‚
â”‚  â”‚  â”‚  CONTAINER  â”‚ â”‚  CONTAINER  â”‚        â”‚    â”‚
â”‚  â”‚  â”‚             â”‚ â”‚             â”‚        â”‚    â”‚
â”‚  â”‚  â”‚ ğŸ¥ Video    â”‚ â”‚ ğŸ’ª Actions  â”‚        â”‚    â”‚
â”‚  â”‚  â”‚ ğŸ–¼ï¸ Vision   â”‚ â”‚ ğŸ—£ï¸ Speech   â”‚        â”‚    â”‚
â”‚  â”‚  â”‚ ğŸ”Š Audio    â”‚ â”‚ âœ‹ Motor    â”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚              â”‚                        â”‚
â”‚         â–¼              â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           BRAIN CONTAINER               â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚ LLM-Neurons  â”‚â”€â”€â”€â”€â”€â–¶â”‚   Corpus     â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  (Parallel)  â”‚      â”‚  Colosseum   â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  (Consensus) â”‚ â”‚    â”‚
â”‚  â”‚         â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚         â”‚                     â”‚          â”‚    â”‚
â”‚  â”‚         â–¼                     â–¼          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚   Message    â”‚      â”‚   Decision   â”‚ â”‚    â”‚
â”‚  â”‚  â”‚   Protocol   â”‚      â”‚    Output    â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚                                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚  Memory      â”‚      â”‚   Frontal    â”‚ â”‚    â”‚
â”‚  â”‚  â”‚   Palace     â”‚â—„â”€â”€â”€â”€â–ºâ”‚    Lobe      â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  (Long-term) â”‚      â”‚ (Executive)  â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        AUTONOMIC CONTAINER              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚   Amygdala   â”‚      â”‚    Sleep     â”‚ â”‚    â”‚
â”‚  â”‚  â”‚  (Emotion)   â”‚      â”‚   Cycle      â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
digital_cortex/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ corpus_colosseum/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention_consensus.py    # Advanced consensus mechanisms
â”‚   â””â”€â”€ colosseum.py              # Consensus mechanism
â”œâ”€â”€ cortex_regions/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ meta_cognition.py         # Self-monitoring system
â”‚   â””â”€â”€ frontal_lobe.py           # Executive decision making
â”œâ”€â”€ learning_center/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ video_acquisition.py      # YouTube video downloading
â”‚   â”œâ”€â”€ multimodal_sensorium.py   # Vision/audio/text processing
â”‚   â”œâ”€â”€ video_learning_orchestrator.py  # Learning coordination
â”‚   â”œâ”€â”€ knowledge_retrieval.py    # Knowledge querying
â”‚   â””â”€â”€ video_learning_container.py     # Container integration
â”œâ”€â”€ memory_palace/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ knowledge_graph.py        # Graph-based memory
â”‚   â”œâ”€â”€ memory_manager.py         # Memory management
â”‚   â””â”€â”€ palace_chain.py           # Chain-based memory
â”œâ”€â”€ sensorium/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ sensorium.py              # Text processing
â”œâ”€â”€ amygdala/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ amygdala.py               # Emotion processing
â”œâ”€â”€ motor_cortex/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ executor.py               # Action execution
â”œâ”€â”€ feedback/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ assessor.py               # Outcome assessment
â”‚   â””â”€â”€ learner.py                # Weight learning
â”œâ”€â”€ sleep/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ consolidator.py           # Memory consolidation
â”‚   â””â”€â”€ dreamer.py                # Dream generation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ message.py                # Message protocol
â”‚   â”œâ”€â”€ llm_neuron.py             # LLM interface
â”‚   â”œâ”€â”€ confidence_scorer.py      # Advanced confidence scoring
â”‚   â”œâ”€â”€ async_utils.py            # Async utilities
â”‚   â”œâ”€â”€ cache.py                  # Caching system
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â””â”€â”€ model_manager.py          # Model management
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_*.py                 # Comprehensive test suite
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ demo_integration.py           # Full system demo
â”œâ”€â”€ test_neuron_quick.py          # Quick connectivity test
â””â”€â”€ __pycache__/
```

## ğŸ¯ What's Next (Roadmap)

### âœ… **Phase 1: Core Architecture - COMPLETE**
- [x] Message Protocol with validation and factory methods
- [x] Corpus Colosseum consensus mechanism with DBSCAN clustering
- [x] LLM-Neuron interface with confidence extraction
- [x] End-to-end integration demo

### âœ… **Phase 2: Memory Palace Chain - COMPLETE**
- [x] Sequential room creation with hash-based addressing
- [x] Chain traversal for "internal voice" simulation
- [x] Graph-based memory system for richer associations
- [x] Integration with Corpus Colosseum outputs

### âœ… **Phase 3: Feedback Cycle - COMPLETE**
- [x] Motor Cortex (action executor) for task execution
- [x] Outcome Assessment Module for performance evaluation
- [x] Weight Update Mechanism with temporal credit assignment
- [x] Learning integration across all brain regions

### âœ… **Phase 4: Sleep Cycle - COMPLETE**
- [x] Dream branch spawning with random walks
- [x] Learning branch processing and consolidation
- [x] Memory reorganization with meta-memory creation
- [x] Offline learning and memory optimization

### âœ… **Phase 5: Specialized Neural Networks - COMPLETE**
- [x] Sensorium (vision, text processing, multimodal input)
- [x] Amygdala (urgency assessment and emotional processing)
- [x] Frontal Lobe (executive function and decision making)
- [x] Meta-cognition layer for self-monitoring

### âœ… **Phase 6: Digital Body Architecture - COMPLETE**
- [x] Container Architecture with multi-container pod design
- [x] Message Bus for inter-container communication
- [x] Health Monitoring and auto-regulation systems
- [x] Complete embodied AI system integration

### âœ… **Phase 7: YouTube Learning System - COMPLETE**
- [x] Video Acquisition with YouTube downloading and frame extraction
- [x] Multimodal Sensorium (Vision/Audio/Text parallel processing)
- [x] Learning Orchestration with consensus synthesis
- [x] Knowledge Extraction and structured learning
- [x] Memory Integration with existing brain architecture
- [x] Knowledge Retrieval and querying system

### ğŸ”„ **Phase 8: Multi-Agent Collaboration (In Progress)**
- [ ] Agent communication protocols
- [ ] Task decomposition and distribution
- [ ] Collaborative problem-solving
- [ ] Agent specialization and role assignment

## ğŸ§ª Testing

The system has been validated with:
- âœ… Message protocol serialization/deserialization
- âœ… Corpus Colosseum consensus with mock data
- âœ… LLM-Neuron connectivity to Ollama
- âœ… End-to-end integration with real LLMs
- âœ… Snake vs Garden Hose scenario (white paper example)
- âœ… Memory Palace chain operations
- âœ… Feedback cycle weight updates
- âœ… Sleep cycle memory consolidation
- âœ… Digital Body container architecture
- âœ… YouTube Learning System multimodal processing
- âœ… Video knowledge extraction and retrieval

## ğŸ“ Key Features

- **Local-First**: All processing happens on your machine
- **Modular**: Each component can be tested and improved independently
- **Extensible**: Easy to add new neuron types or consensus algorithms
- **Bio-Inspired**: Architecture mirrors actual brain function
- **Transparent**: Full visibility into decision-making process
- **ğŸ¥ Video Learning**: Can watch and learn from YouTube videos
- **ğŸ§  Multimodal**: Processes vision, audio, and text simultaneously
- **ğŸ“š Knowledge Base**: Builds structured knowledge from video content
- **ğŸ—ï¸ Embodied AI**: Complete digital body with sensory-motor integration

## ğŸ”¬ Technical Details

### Consensus Mechanism
The Corpus Colosseum uses DBSCAN clustering to find where multiple neuron outputs converge:
1. Embed each neuron's output in vector space
2. Apply DBSCAN to find clusters
3. Score clusters by: `size Ã— avg_confidence`
4. Select highest-scoring cluster
5. Return highest-confidence message from winning cluster

### Confidence Extraction
LLM-Neurons can extract confidence scores from model outputs:
- Pattern matching for `[CONFIDENCE: 0.XX]` tags
- Fallback to default 0.5 if not found
- Clamped to [0.0, 1.0] range

## ğŸ“š References

- [Digital Cortex White Paper](Digital_Cortex_White_Paper.md)
- [AGI Execution Plan](agi%20whitepaper)
- [Memory Palace NN Repository](https://github.com/luckyduckcode/memory-palace-nueral-network-only)

## ğŸ¤ Contributing

This is an active research project. The architecture is designed to be:
- Experimentally validated
- Iteratively improved
- Empirically tested

## ğŸ“„ License

See project repository for license information.

---

**Status**: Digital Body Architecture âœ… | YouTube Learning System âœ… | Multi-Agent Collaboration ğŸ”„

**Latest Achievement**: Chappy can now watch educational YouTube videos and learn from them through sophisticated multimodal processing! ğŸ¥ğŸ§ 
